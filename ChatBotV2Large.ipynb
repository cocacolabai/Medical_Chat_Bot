{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from spacy.lang.en import English\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras import utils\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data from a medical Q&A dataset containing over 135,000 questions and answer pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"healthtapQAs.json\") as f: # This is the biggest database - 137,052 questions and answers\n",
    "    my_data1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zirconium dental implants. how common is it used now. is there any advantages or benefits over titanium implants. cons & pros please. thanks.\n",
      "a majority of the dental implants placed are titanium. they are highly successful with many years use ; many studies much lower in cost ; have many restorative options. zirconia implants are newer fewer studies on success lesser restorative options. however they can be more aesthetic in certain anterior(front) situations. let your dentist/oral surgeon chose what they feel will be best for you.\n"
     ]
    }
   ],
   "source": [
    "the_answers = list()\n",
    "the_questions = list()\n",
    "\n",
    "count = 0\n",
    "count1 = 0\n",
    "\n",
    "for i in my_data1:\n",
    "    the_answers.append(i['answer'])\n",
    "    count += 1\n",
    "\n",
    "for i in my_data1:\n",
    "    the_questions.append(i['question'])\n",
    "    count1 += 1\n",
    "    \n",
    "print(the_questions[0])\n",
    "print(the_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data to get rid of some contractions \n",
    "## In the future I am thinking about applying this to each question written by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zirconium dental implants. how common is it used now. is there any advantages or benefits over titanium implants. cons & pros please. thanks.', 'zirconium dental implants. how common is it used now. is there any advantages or benefits over titanium implants. cons & pros please. thanks.', 'zirconium dental implants. how common is it used now. is there any advantages or benefits over titanium implants. cons & pros please. thanks.', 'zirconium dental implants. how common is it used now. is there any advantages or benefits over titanium implants. cons & pros please. thanks.']\n",
      "['zirconium dental implants how common is it used now is there any advantages or benefits over titanium implants cons & pros please thanks', 'zirconium dental implants how common is it used now is there any advantages or benefits over titanium implants cons & pros please thanks', 'zirconium dental implants how common is it used now is there any advantages or benefits over titanium implants cons & pros please thanks', 'zirconium dental implants how common is it used now is there any advantages or benefits over titanium implants cons & pros please thanks']\n"
     ]
    }
   ],
   "source": [
    "answers = list()\n",
    "questions = list()\n",
    "for i in the_answers:\n",
    "    text = clean_text(i)\n",
    "    answers.append(text)\n",
    "\n",
    "for i in the_questions:\n",
    "    text = clean_text(i)\n",
    "    questions.append(text)\n",
    "    \n",
    "print(the_questions[:4])\n",
    "print(questions[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding 'START' and 'END' to the beginnings and endings of the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and the data on zirconia implants is much more limited', 'dental implants when loaded transfer stress to the bone which is a good thing zirconium is a ceramic that is very hard and actually transfers less stress to the bone and can actually lead to bone atrophy  the you dont use it you lose it concept', 'stick with what we know works  titanium let the research continue with the zirconium more answers will come as more research is done hard to beat 50 years of success always looking for improvement though']\n"
     ]
    }
   ],
   "source": [
    "print(answers[1:4])\n",
    "labeled_answers = list()\n",
    "for i in range(len(answers)):\n",
    "    labeled_answers.append('<START>' + answers[i] + '<END>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing words in both the questions and answers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab list has: 87816 elements\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(questions + labeled_answers)\n",
    "print(\"Vocab list has: {} elements\".format(len(tokenizer.word_index)))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Questions Data\n",
    "## - Tokenize words\n",
    "## - Create word integer index\n",
    "## - Turn text to number sequence and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence_questions is: 64\n",
      "Sequences are of dimensions: (137052, 64)\n",
      "Example sequence: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 15221  1308   201    45   140     4\n",
      "    11   155   136     4    38    65  3576    21   830   128  8161   201\n",
      "  2943  3163   149  1030]\n",
      "Example of WordIndex: {'the': 1, 'to': 2, 'a': 3, 'is': 4}\n"
     ]
    }
   ],
   "source": [
    "sequences_questions = tokenizer.texts_to_sequences(questions)\n",
    "max_seq_len = max([len(x) for x in sequences_questions])\n",
    "print(\"Longest sequence_questions is: {}\".format(max_seq_len))\n",
    "padded_questions = pad_sequences(sequences_questions, maxlen=max_seq_len)\n",
    "# array of padded sequences for questions\n",
    "encoder_input_data = np.array(padded_questions) \n",
    "\n",
    "print(\"Sequences are of dimensions: {}\".format(encoder_input_data.shape))\n",
    "print(\"Example sequence: {}\".format(encoder_input_data[0]))\n",
    "print(\"Example of WordIndex: {}\".format(dict(itertools.islice(word_index.items(), 4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Answers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence_answers is: 100\n",
      "Sequences are of dimensions: (137052, 100)\n",
      "Example sequence: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 15221  1308   201    45   140     4\n",
      "    11   155   136     4    38    65  3576    21   830   128  8161   201\n",
      "  2943  3163   149  1030]\n",
      "Example of WordIndex: {'the': 1, 'to': 2, 'a': 3, 'is': 4}\n"
     ]
    }
   ],
   "source": [
    "sequences_answers = tokenizer.texts_to_sequences(labeled_answers)\n",
    "max_seq_lenAns = 100\n",
    "# max_seq_lenAns = max([len(x) for x in sequences_answers])\n",
    "print(\"Longest sequence_answers is: {}\".format(max_seq_lenAns))\n",
    "padded_answers = pad_sequences(sequences_answers, maxlen=max_seq_lenAns)\n",
    "# array of padded sequences for questions\n",
    "decoder_input_data = np.array(padded_answers) \n",
    "\n",
    "print(\"Sequences are of dimensions: {}\".format(decoder_input_data.shape))\n",
    "print(\"Example sequence: {}\".format(encoder_input_data[0]))\n",
    "print(\"Example of WordIndex: {}\".format(dict(itertools.islice(word_index.items(), 4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment with the output dim \n",
    "output_dim = 200\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(max_words, output_dim, mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(output_dim, return_state=True) (encoder_embedding)\n",
    "# encoder_states = state_h, state_c\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(max_words, output_dim, mask_zero=True) (decoder_inputs)\n",
    "decoder_LSTM = LSTM(output_dim, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(max_words, activation='softmax')\n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 200)    4000000     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, None, 200)    4000000     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, 200), (None, 320800      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, None, 200),  320800      embedding_13[0][0]               \n",
      "                                                                 lstm_11[0][1]                    \n",
      "                                                                 lstm_11[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 20000)  4020000     lstm_12[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,661,600\n",
      "Trainable params: 12,661,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the word START from the answer sequences and padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_seq = sequences_answers\n",
    "for i in range(len(decoder_output_seq)):\n",
    "    decoder_output_seq[i] = decoder_output_seq[i][1:]\n",
    "\n",
    "target_data = pad_sequences(decoder_output_seq, max_seq_lenAns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling then splitting the input data and target data 80:20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input:  (137052, 64)\n",
      "Decoder Input:  (137052, 100)\n",
      "Target Data Input:  (137052, 100)\n"
     ]
    }
   ],
   "source": [
    "# should all share the shape (137052, _)\n",
    "print(\"Encoder Input: \", encoder_input_data.shape)\n",
    "print(\"Decoder Input: \", decoder_input_data.shape)\n",
    "print(\"Target Data Input: \", target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109641.6\n",
      "0.7989668155152789\n"
     ]
    }
   ],
   "source": [
    "eighty = 0.8 * encoder_input_data.shape[0]\n",
    "print(eighty)\n",
    "train_perc = 109500/encoder_input_data.shape[0]\n",
    "print(train_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I will be fitting the model in portion sizes of 300 I am going\n",
    "# use 109,500 which is approximately 80% (actually 79.897%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137052, 64)\n",
      "(137052, 100)\n",
      "(137052, 100)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input_data.shape[0])\n",
    "np.random.shuffle(indices) \n",
    "# EID = encoder_input_data\n",
    "shuffled_EID = encoder_input_data[indices]\n",
    "shuffled_DID = decoder_input_data[indices]\n",
    "shuffled_target = target_data[indices]\n",
    "\n",
    "print(shuffled_EID.shape)\n",
    "print(shuffled_DID.shape)\n",
    "print(shuffled_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109500, 64)\n",
      "(109500, 100)\n",
      "(109500, 100)\n",
      "(27552, 64)\n",
      "(27552, 100)\n",
      "(27552, 100)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0    10    75   977   641   185   172  2815\n",
      "    75 13875    48  4032  5894   151   227  4074     4    11   433   727\n",
      "    10    71    42   379]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   7 285  19   8]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 285  19   8]\n"
     ]
    }
   ],
   "source": [
    "train_encoder_data = shuffled_EID[0:109500] \n",
    "train_decoder_data = shuffled_DID[0:109500]\n",
    "train_target_data = shuffled_target[0:109500]\n",
    "\n",
    "test_encoder_data = shuffled_EID[109500: ] \n",
    "test_decoder_data = shuffled_DID[109500: ]\n",
    "test_target_data = shuffled_target[109500: ]\n",
    "\n",
    "print(train_encoder_data.shape)\n",
    "print(train_decoder_data.shape)\n",
    "print(train_target_data.shape)\n",
    "print(test_encoder_data.shape)\n",
    "print(test_decoder_data.shape)\n",
    "print(test_target_data.shape)\n",
    "\n",
    "print(train_encoder_data[1])\n",
    "print(train_decoder_data[1])\n",
    "print(train_target_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model in portions since the OneHotEncoded Answers are too large to be held in computer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently--> 0:300\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - ETA: 58s - loss: 9.90 - ETA: 42s - loss: 9.90 - ETA: 33s - loss: 9.89 - ETA: 27s - loss: 9.84 - ETA: 21s - loss: 9.67 - ETA: 16s - loss: 9.45 - ETA: 11s - loss: 9.21 - ETA: 6s - loss: 8.9857 - ETA: 1s - loss: 8.783 - 45s 150ms/step - loss: 8.7043\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - ETA: 37s - loss: 6.77 - ETA: 32s - loss: 6.77 - ETA: 28s - loss: 6.77 - ETA: 24s - loss: 6.74 - ETA: 19s - loss: 6.69 - ETA: 15s - loss: 6.69 - ETA: 10s - loss: 6.71 - ETA: 6s - loss: 6.6975 - ETA: 1s - loss: 6.692 - 43s 142ms/step - loss: 6.6787\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-d0808827d3fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#     print(output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#     print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_X1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_X2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tenth = 300\n",
    "times_completed = int(train_target_data.shape[0]/300)\n",
    "for i in range(5):\n",
    "    current = i * tenth\n",
    "    nextone = (i+1) * tenth\n",
    "    print(\"Currently--> {}:{}\".format(current, nextone))\n",
    "    train_X1 = train_encoder_data[current:nextone]\n",
    "    train_X2 = train_decoder_data[current:nextone]\n",
    "    portion = train_target_data[current:nextone]\n",
    "    output = utils.to_categorical(portion, max_words)\n",
    "#     print(train_X1.shape)\n",
    "#     print(train_X2.shape)\n",
    "#     print(output.shape)\n",
    "#     print(output)\n",
    "    model.fit([train_X1 , train_X2], output, batch_size=32, epochs=20 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"chat_botV2Large.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
